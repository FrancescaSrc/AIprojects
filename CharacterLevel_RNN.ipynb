{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CharacterLevel_RNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FrancescaSrc/AIprojects/blob/master/CharacterLevel_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-xIkzdtc6sR5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Character-Level LSTM in PyTorch\n",
        "\n",
        "In this notebook, I'll construct a character-level LSTM with PyTorch. The network will train character by character on some text, then generate new text character by character. I will train on Dante's Divina Commedia. **This model will be able to generate new text based on the text from the book!**\n",
        "\n",
        "This network is based off of Andrej Karpathy's [post on RNNs](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) and [implementation in Torch](https://github.com/karpathy/char-rnn). Below is the general architecture of the character-wise RNN.\n"
      ]
    },
    {
      "metadata": {
        "id": "bgZNCSGiX4l0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vb0PWyY_V14W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g0TF0qRhU5D4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.1-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IUniTIXjcbKR",
        "colab_type": "code",
        "outputId": "b8b3bc6d-be36-49ba-ac4a-90c8f67d8829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "!ls 'drive/My Drive/Colabs/data/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "anna.txt  dante.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yeOihXrMX6X1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with open('drive/My Drive/Colabs/data/dante.txt', 'r') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9Yo9LAOr6ZVZ",
        "colab_type": "code",
        "outputId": "d0347879-9b6e-47d1-face-c9212fd8f7c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "text[:100]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeff\\nLA DIVINA COMMEDIA DI DANTE ALIGHIERI\\n\\nIncipit Comoedia Dantis Alagherii,\\nFlorentini natione, non '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "metadata": {
        "id": "y8DqVco67H4z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Create a couple **dictionaries** to convert the characters to and from integers. Encoding the characters as integers makes it easier to use as input in the network."
      ]
    },
    {
      "metadata": {
        "id": "5TMEhXmc7R4C",
        "colab_type": "code",
        "outputId": "32747d54-a895-4248-8f5d-90b138af39f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "cell_type": "code",
      "source": [
        "# encode the text and map each character to an integer and vice versa\n",
        "\n",
        "# we create two dictionaries:\n",
        "# 1. int2char, which maps integers to characters\n",
        "# 2. char2int, which maps characters to unique integers\n",
        "chars= tuple(set(text)) #this is the vocabulary, the set function will get rid of duplicates, selecting unique charaters\n",
        "print('chars', chars[:100])\n",
        "int2char= dict(enumerate(chars))\n",
        "char2int = {ch: ii for ii, ch in int2char.items()}\n",
        "print('char2int', char2int)\n",
        "\n",
        "# encode the text\n",
        "encoded = np.array([char2int[ch] for ch in text])\n",
        "print('encoded', encoded[:100])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chars ('L', '-', 'H', 'z', '`', 'ň', 's', ',', 'f', 'ó', 'h', 'N', 'ů', '#', 'i', 'J', '>', '\\ufeff', 'e', 'l', 'B', 'I', ')', ' ', 'p', '?', 'Q', 'P', 'Z', '3', 'U', 'D', 'R', 'V', 'w', 'o', 'b', '@', 'x', 't', 'c', '8', 'y', '0', 'G', 'M', '\\n', 'A', ':', '2', 'j', 'O', 'a', '<', '7', '[', 'd', 'W', 'X', 'ŕ', 'q', '1', '(', '/', '!', 'F', 'm', 'Ť', '9', 'ě', 'C', 'č', 'v', 'S', ']', 'E', 'ť', '.', '\"', 'k', 'é', 'u', ';', 'r', 'n', 'g', \"'\", 'ď', 'T')\n",
            "char2int {'L': 0, '-': 1, 'H': 2, 'z': 3, '`': 4, 'ň': 5, 's': 6, ',': 7, 'f': 8, 'ó': 9, 'h': 10, 'N': 11, 'ů': 12, '#': 13, 'i': 14, 'J': 15, '>': 16, '\\ufeff': 17, 'e': 18, 'l': 19, 'B': 20, 'I': 21, ')': 22, ' ': 23, 'p': 24, '?': 25, 'Q': 26, 'P': 27, 'Z': 28, '3': 29, 'U': 30, 'D': 31, 'R': 32, 'V': 33, 'w': 34, 'o': 35, 'b': 36, '@': 37, 'x': 38, 't': 39, 'c': 40, '8': 41, 'y': 42, '0': 43, 'G': 44, 'M': 45, '\\n': 46, 'A': 47, ':': 48, '2': 49, 'j': 50, 'O': 51, 'a': 52, '<': 53, '7': 54, '[': 55, 'd': 56, 'W': 57, 'X': 58, 'ŕ': 59, 'q': 60, '1': 61, '(': 62, '/': 63, '!': 64, 'F': 65, 'm': 66, 'Ť': 67, '9': 68, 'ě': 69, 'C': 70, 'č': 71, 'v': 72, 'S': 73, ']': 74, 'E': 75, 'ť': 76, '.': 77, '\"': 78, 'k': 79, 'é': 80, 'u': 81, ';': 82, 'r': 83, 'n': 84, 'g': 85, \"'\": 86, 'ď': 87, 'T': 88}\n",
            "encoded [17 46  0 47 23 31 21 33 21 11 47 23 70 51 45 45 75 31 21 47 23 31 21 23\n",
            " 31 47 11 88 75 23 47  0 21 44  2 21 75 32 21 46 46 21 84 40 14 24 14 39\n",
            " 23 70 35 66 35 18 56 14 52 23 31 52 84 39 14  6 23 47 19 52 85 10 18 83\n",
            " 14 14  7 46 65 19 35 83 18 84 39 14 84 14 23 84 52 39 14 35 84 18  7 23\n",
            " 84 35 84 23]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SA1BnjVDB-Ua",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def one_hot_encode(arr, n_labels):\n",
        "    \n",
        "    # Initialize the the encoded array [* means all the args in an array] \n",
        "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
        "    \n",
        "    # Fill the appropriate elements with ones\n",
        "    # np.arange retuns an array ex. np.arange(3,7) returns array([3, 4, 5, 6])\n",
        "    # numpy.arange([start, ]stop, [step, ] dtype=None)\n",
        "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
        "    \n",
        "    # Finally reshape it to get back to the original array\n",
        "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qiPkS-3nX97F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_batches(arr, batch_size, seq_length):\n",
        "    '''Create a generator that returns batches of size\n",
        "       batch_size x seq_length from arr.\n",
        "       \n",
        "       Arguments\n",
        "       ---------\n",
        "       arr: Array you want to make batches from\n",
        "       batch_size: Batch size, the number of sequences per batch\n",
        "       seq_length: Number of encoded chars in a sequence\n",
        "    '''\n",
        "    \n",
        "    ## TODO: Get the number of batches we can make\n",
        "    batch_size_total = batch_size * seq_length\n",
        "    n_batch = len(arr)//batch_size_total\n",
        "    ## TODO: Keep only enough characters to make full batches\n",
        "    arr = arr[:n_batch*batch_size_total]\n",
        "    \n",
        "    ## TODO: Reshape into batch_size rows\n",
        "    arr = arr.reshape((batch_size,-1))\n",
        "    \n",
        "    ## TODO: Iterate over the batches using a window of size seq_length\n",
        "    for n in range(0, arr.shape[1], seq_length):\n",
        "        # The features\n",
        "        x = arr[:, n:n+seq_length]\n",
        "        # The targets, shifted by one\n",
        "        y = np.zeros_like(x)\n",
        "        try:\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
        "        except IndexError:\n",
        "            #make the last element of y equal to the first element in the array\n",
        "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
        "        \n",
        "       \n",
        "       \n",
        "        yield x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MdEkCwl7aEJO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# test\n",
        "batches = get_batches(encoded, 8, 50)\n",
        "x, y = next(batches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0F9e0MjnNyEt",
        "colab_type": "code",
        "outputId": "e237f552-fb4f-4617-cd01-a3b26b11fc51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "cell_type": "code",
      "source": [
        "# printing out the first 10 items in a sequence\n",
        "print('x\\n', x[:10, :10])\n",
        "print('\\ny\\n', y[:10, :10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x\n",
            " [[17 46  0 47 23 31 21 33 21 11]\n",
            " [10 18 23 72 14 84 40 14 46 23]\n",
            " [ 8 19 14 39 39 35  7 46 23 23]\n",
            " [72 83 52 23 19 86 75 83 66 35]\n",
            " [10 86 18  4 23 66 35 39 35 23]\n",
            " [ 6 35  8  8 14 52 39 52 23 18]\n",
            " [23 40 35 66 86 81 35 66 23  6]\n",
            " [23 24 18 83 23 66 14 85 19 14]]\n",
            "\n",
            "y\n",
            " [[46  0 47 23 31 21 33 21 11 47]\n",
            " [18 23 72 14 84 40 14 46 23 23]\n",
            " [19 14 39 39 35  7 46 23 23  6]\n",
            " [83 52 23 19 86 75 83 66 35 23]\n",
            " [86 18  4 23 66 35 39 35 23  6]\n",
            " [35  8  8 14 52 39 52 23 18 23]\n",
            " [40 35 66 86 81 35 66 23  6 86]\n",
            " [24 18 83 23 66 14 85 19 14 35]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "icl0OvxsXDx1",
        "colab_type": "code",
        "outputId": "4cd2d81d-f295-4ca8-e31c-b28ce5b89c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# check if GPU is available\n",
        "train_on_gpu = torch.cuda.is_available()\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU!')\n",
        "else: \n",
        "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yXFoQW5VOxoE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CharRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
        "                               drop_prob=0.5, lr=0.001):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        # creating character dictionaries\n",
        "        self.chars = tokens\n",
        "        self.int2char = dict(enumerate(self.chars))\n",
        "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
        "        \n",
        "        ## TODO: define the LSTM\n",
        "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## TODO: define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## TODO: define the final, fully-connected output layer\n",
        "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "                \n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        r_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        ## pass through a dropout layer\n",
        "        out = self.dropout(r_output)\n",
        "        \n",
        "        # Stack up LSTM outputs using view\n",
        "        # you may need to use contiguous to reshape the output\n",
        "        out = out.contiguous().view(-1, self.n_hidden)\n",
        "        \n",
        "        ## put x through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "        \n",
        "       \n",
        "        \n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OywlVcFBe6D1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#train the RNN\n",
        "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
        "    ''' Training a network \n",
        "    \n",
        "        Arguments\n",
        "        ---------\n",
        "        \n",
        "        net: CharRNN network\n",
        "        data: text data to train the network\n",
        "        epochs: Number of epochs to train\n",
        "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
        "        seq_length: Number of character steps per mini-batch\n",
        "        lr: learning rate\n",
        "        clip: gradient clipping\n",
        "        val_frac: Fraction of data to hold out for validation\n",
        "        print_every: Number of steps for printing training and validation loss\n",
        "    \n",
        "    '''\n",
        "    net.train()\n",
        "    \n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # create training and validation data\n",
        "    val_idx = int(len(data)*(1-val_frac))\n",
        "    data, val_data = data[:val_idx], data[val_idx:]\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "    n_chars = len(net.chars)\n",
        "    for e in range(epochs):\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(data, batch_size, seq_length):\n",
        "            counter += 1\n",
        "            \n",
        "            # One-hot encode our data and make them Torch tensors\n",
        "            x = one_hot_encode(x, n_chars)\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            if(train_on_gpu):\n",
        "                inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # Creating new variables for the hidden state, otherwise\n",
        "            # we'd backprop through the entire training history\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(batch_size*seq_length))\n",
        "            loss.backward()\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "            opt.step()\n",
        "            \n",
        "            # loss stats\n",
        "            if counter % print_every == 0:\n",
        "                # Get validation loss\n",
        "                val_h = net.init_hidden(batch_size)\n",
        "                val_losses = []\n",
        "                net.eval()\n",
        "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
        "                    # One-hot encode our data and make them Torch tensors\n",
        "                    x = one_hot_encode(x, n_chars)\n",
        "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                    \n",
        "                    # Creating new variables for the hidden state, otherwise\n",
        "                    # we'd backprop through the entire training history\n",
        "                    val_h = tuple([each.data for each in val_h])\n",
        "                    \n",
        "                    inputs, targets = x, y\n",
        "                    if(train_on_gpu):\n",
        "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                    output, val_h = net(inputs, val_h)\n",
        "                    val_loss = criterion(output, targets.view(batch_size*seq_length))\n",
        "                \n",
        "                    val_losses.append(val_loss.item())\n",
        "                \n",
        "                net.train() # reset to train mode after iterationg through validation data\n",
        "                \n",
        "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                      \"Step: {}...\".format(counter),\n",
        "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses))) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lAfXM-MSWnJl",
        "colab_type": "code",
        "outputId": "0059297c-bf2b-4e98-f4c6-7d09ab683d32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(train_on_gpu)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kKnx9Bp6ULch",
        "colab_type": "code",
        "outputId": "3c43f750-69a4-415e-cc0c-839d7a93fad5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "# define and print the net\n",
        "n_hidden=1024\n",
        "n_layers=2\n",
        "\n",
        "net = CharRNN(chars, n_hidden, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CharRNN(\n",
            "  (lstm): LSTM(89, 1024, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.5)\n",
            "  (fc): Linear(in_features=1024, out_features=89, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EK2EMjZqUkT-",
        "colab_type": "code",
        "outputId": "7e266e41-8a2a-4eb0-f8a9-9fb0c84e7d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1327
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size =128\n",
        "seq_length = 100\n",
        "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
        "\n",
        "# train the model\n",
        "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/20... Step: 10... Loss: 3.0848... Val Loss: 3.0480\n",
            "Epoch: 1/20... Step: 20... Loss: 3.0492... Val Loss: 3.0360\n",
            "Epoch: 1/20... Step: 30... Loss: 3.0319... Val Loss: 3.0247\n",
            "Epoch: 2/20... Step: 40... Loss: 3.0526... Val Loss: 3.0196\n",
            "Epoch: 2/20... Step: 50... Loss: 2.9987... Val Loss: 3.0078\n",
            "Epoch: 2/20... Step: 60... Loss: 3.0206... Val Loss: 2.9924\n",
            "Epoch: 2/20... Step: 70... Loss: 2.9553... Val Loss: 2.9466\n",
            "Epoch: 3/20... Step: 80... Loss: 2.8286... Val Loss: 2.8419\n",
            "Epoch: 3/20... Step: 90... Loss: 2.6757... Val Loss: 2.6717\n",
            "Epoch: 3/20... Step: 100... Loss: 2.5327... Val Loss: 2.5616\n",
            "Epoch: 3/20... Step: 110... Loss: 2.4485... Val Loss: 2.4827\n",
            "Epoch: 4/20... Step: 120... Loss: 2.3864... Val Loss: 2.4080\n",
            "Epoch: 4/20... Step: 130... Loss: 2.3184... Val Loss: 2.3621\n",
            "Epoch: 4/20... Step: 140... Loss: 2.2994... Val Loss: 2.3375\n",
            "Epoch: 4/20... Step: 150... Loss: 2.2488... Val Loss: 2.3042\n",
            "Epoch: 5/20... Step: 160... Loss: 2.1909... Val Loss: 2.2534\n",
            "Epoch: 5/20... Step: 170... Loss: 2.1501... Val Loss: 2.2235\n",
            "Epoch: 5/20... Step: 180... Loss: 2.1365... Val Loss: 2.1959\n",
            "Epoch: 5/20... Step: 190... Loss: 2.0836... Val Loss: 2.1563\n",
            "Epoch: 6/20... Step: 200... Loss: 2.0674... Val Loss: 2.1324\n",
            "Epoch: 6/20... Step: 210... Loss: 2.0325... Val Loss: 2.1060\n",
            "Epoch: 6/20... Step: 220... Loss: 2.0199... Val Loss: 2.0921\n",
            "Epoch: 6/20... Step: 230... Loss: 1.9784... Val Loss: 2.0653\n",
            "Epoch: 7/20... Step: 240... Loss: 1.9599... Val Loss: 2.0405\n",
            "Epoch: 7/20... Step: 250... Loss: 1.9397... Val Loss: 2.0177\n",
            "Epoch: 7/20... Step: 260... Loss: 1.9350... Val Loss: 2.0060\n",
            "Epoch: 7/20... Step: 270... Loss: 1.9212... Val Loss: 1.9839\n",
            "Epoch: 8/20... Step: 280... Loss: 1.8966... Val Loss: 1.9901\n",
            "Epoch: 8/20... Step: 290... Loss: 1.8845... Val Loss: 1.9592\n",
            "Epoch: 8/20... Step: 300... Loss: 1.8657... Val Loss: 1.9451\n",
            "Epoch: 8/20... Step: 310... Loss: 1.8320... Val Loss: 1.9301\n",
            "Epoch: 9/20... Step: 320... Loss: 1.8247... Val Loss: 1.9177\n",
            "Epoch: 9/20... Step: 330... Loss: 1.8128... Val Loss: 1.9037\n",
            "Epoch: 9/20... Step: 340... Loss: 1.7956... Val Loss: 1.8941\n",
            "Epoch: 9/20... Step: 350... Loss: 1.8021... Val Loss: 1.8866\n",
            "Epoch: 10/20... Step: 360... Loss: 1.7796... Val Loss: 1.8732\n",
            "Epoch: 10/20... Step: 370... Loss: 1.7462... Val Loss: 1.8628\n",
            "Epoch: 10/20... Step: 380... Loss: 1.7610... Val Loss: 1.8472\n",
            "Epoch: 10/20... Step: 390... Loss: 1.7676... Val Loss: 1.8414\n",
            "Epoch: 11/20... Step: 400... Loss: 1.7519... Val Loss: 1.8259\n",
            "Epoch: 11/20... Step: 410... Loss: 1.7352... Val Loss: 1.8265\n",
            "Epoch: 11/20... Step: 420... Loss: 1.7144... Val Loss: 1.8116\n",
            "Epoch: 12/20... Step: 430... Loss: 1.7565... Val Loss: 1.8039\n",
            "Epoch: 12/20... Step: 440... Loss: 1.6722... Val Loss: 1.7907\n",
            "Epoch: 12/20... Step: 450... Loss: 1.6865... Val Loss: 1.7802\n",
            "Epoch: 12/20... Step: 460... Loss: 1.6545... Val Loss: 1.7714\n",
            "Epoch: 13/20... Step: 470... Loss: 1.6562... Val Loss: 1.7647\n",
            "Epoch: 13/20... Step: 480... Loss: 1.6553... Val Loss: 1.7502\n",
            "Epoch: 13/20... Step: 490... Loss: 1.6479... Val Loss: 1.7481\n",
            "Epoch: 13/20... Step: 500... Loss: 1.6220... Val Loss: 1.7370\n",
            "Epoch: 14/20... Step: 510... Loss: 1.6220... Val Loss: 1.7276\n",
            "Epoch: 14/20... Step: 520... Loss: 1.6152... Val Loss: 1.7264\n",
            "Epoch: 14/20... Step: 530... Loss: 1.6193... Val Loss: 1.7162\n",
            "Epoch: 14/20... Step: 540... Loss: 1.5900... Val Loss: 1.7104\n",
            "Epoch: 15/20... Step: 550... Loss: 1.5566... Val Loss: 1.7013\n",
            "Epoch: 15/20... Step: 560... Loss: 1.5565... Val Loss: 1.7002\n",
            "Epoch: 15/20... Step: 570... Loss: 1.5708... Val Loss: 1.6922\n",
            "Epoch: 15/20... Step: 580... Loss: 1.5506... Val Loss: 1.6868\n",
            "Epoch: 16/20... Step: 590... Loss: 1.5670... Val Loss: 1.6791\n",
            "Epoch: 16/20... Step: 600... Loss: 1.5666... Val Loss: 1.6709\n",
            "Epoch: 16/20... Step: 610... Loss: 1.5586... Val Loss: 1.6669\n",
            "Epoch: 16/20... Step: 620... Loss: 1.5269... Val Loss: 1.6635\n",
            "Epoch: 17/20... Step: 630... Loss: 1.5205... Val Loss: 1.6566\n",
            "Epoch: 17/20... Step: 640... Loss: 1.5273... Val Loss: 1.6585\n",
            "Epoch: 17/20... Step: 650... Loss: 1.5341... Val Loss: 1.6505\n",
            "Epoch: 17/20... Step: 660... Loss: 1.5211... Val Loss: 1.6476\n",
            "Epoch: 18/20... Step: 670... Loss: 1.4961... Val Loss: 1.6449\n",
            "Epoch: 18/20... Step: 680... Loss: 1.5111... Val Loss: 1.6350\n",
            "Epoch: 18/20... Step: 690... Loss: 1.5046... Val Loss: 1.6304\n",
            "Epoch: 18/20... Step: 700... Loss: 1.4855... Val Loss: 1.6307\n",
            "Epoch: 19/20... Step: 710... Loss: 1.4814... Val Loss: 1.6257\n",
            "Epoch: 19/20... Step: 720... Loss: 1.4982... Val Loss: 1.6187\n",
            "Epoch: 19/20... Step: 730... Loss: 1.4691... Val Loss: 1.6147\n",
            "Epoch: 19/20... Step: 740... Loss: 1.4476... Val Loss: 1.6164\n",
            "Epoch: 20/20... Step: 750... Loss: 1.4677... Val Loss: 1.6104\n",
            "Epoch: 20/20... Step: 760... Loss: 1.4386... Val Loss: 1.6109\n",
            "Epoch: 20/20... Step: 770... Loss: 1.4572... Val Loss: 1.6031\n",
            "Epoch: 20/20... Step: 780... Loss: 1.4772... Val Loss: 1.6013\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HN-sUYQVZL-o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# change the name, for saving multiple files\n",
        "model_name = 'rnn_20_epoch1.net'\n",
        "\n",
        "checkpoint = {'n_hidden': net.n_hidden,\n",
        "              'n_layers': net.n_layers,\n",
        "              'state_dict': net.state_dict(),\n",
        "              'tokens': net.chars}\n",
        "\n",
        "with open(model_name, 'wb') as f:\n",
        "    torch.save(checkpoint, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UxZq0SyCbT9u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(net, char, h=None, top_k=None):\n",
        "        ''' Given a character, predict the next character.\n",
        "            Returns the predicted character and the hidden state.\n",
        "        '''\n",
        "        \n",
        "        # tensor inputs\n",
        "        x = np.array([[net.char2int[char]]])\n",
        "        x = one_hot_encode(x, len(net.chars))\n",
        "        inputs = torch.from_numpy(x)\n",
        "        \n",
        "        if(train_on_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        # detach hidden state from history\n",
        "        h = tuple([each.data for each in h])\n",
        "        # get the output of the model\n",
        "        out, h = net(inputs, h)\n",
        "\n",
        "        # get the character probabilities\n",
        "        p = F.softmax(out, dim=1).data\n",
        "        if(train_on_gpu):\n",
        "            p = p.cpu() # move to cpu\n",
        "        \n",
        "        # get top characters\n",
        "        if top_k is None:\n",
        "            top_ch = np.arange(len(net.chars))\n",
        "        else:\n",
        "            p, top_ch = p.topk(top_k)\n",
        "            top_ch = top_ch.numpy().squeeze()\n",
        "        \n",
        "        # select the likely next character with some element of randomness\n",
        "        p = p.numpy().squeeze()\n",
        "        char = np.random.choice(top_ch, p=p/p.sum())\n",
        "        \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return net.int2char[char], h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CjlUBPpobYmI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(net, size, prime='In', top_k=None):\n",
        "        \n",
        "    if(train_on_gpu):\n",
        "        net.cuda()\n",
        "    else:\n",
        "        net.cpu()\n",
        "    \n",
        "    net.eval() # eval mode\n",
        "    \n",
        "    # First off, run through the prime characters\n",
        "    chars = [ch for ch in prime]\n",
        "    h = net.init_hidden(1)\n",
        "    for ch in prime:\n",
        "        char, h = predict(net, ch, h, top_k=top_k)\n",
        "\n",
        "    chars.append(char)\n",
        "    \n",
        "    # Now pass in the previous character and get a new one\n",
        "    for ii in range(size):\n",
        "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
        "        chars.append(char)\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4L0TiP3hbcbR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
        "with open('rnn_20_epoch1.net', 'rb') as f:\n",
        "    checkpoint = torch.load(f)\n",
        "    \n",
        "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
        "loaded.load_state_dict(checkpoint['state_dict'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NUZUni3T9OQZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Now let's see the results!**"
      ]
    },
    {
      "metadata": {
        "id": "H0GVE_o6bfXQ",
        "colab_type": "code",
        "outputId": "e10666fa-9e94-4c78-bd51-aa6ec51377a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1126
        }
      },
      "cell_type": "code",
      "source": [
        "# Sample using a loaded model\n",
        "print(sample(loaded, 2000, top_k=5, prime=\"Nel\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nel mal pion sen pel perso\n",
            "  pia` ch'i l'incrima e dan pii` san don den del posti.\n",
            "\n",
            "Le sosti sosenti selto a sonno sisini.\n",
            "\n",
            "La` siccesti che, sol piana a la molte,\n",
            "  che ' do piela den col santi, si ciute,\n",
            "  che puu` cale`t che di contarti ch'asto.\n",
            "\n",
            "Lo dior con du came si contra paco,\n",
            "  che si conte li sorto arta l'irti,\n",
            "  polte di porcen seser per puu` sisto.\n",
            "\n",
            "Por si morto soltri do l'ante sucitto.\n",
            "\n",
            "Postu sel di per che s'artri a le pan di festi,\n",
            "  do par se lo saltro donor puarte,\n",
            "  come che di fan den mu perdo che le pione\n",
            "  centro corti sisten la sento che porch'elle,\n",
            "  che 'ldo che di ciu` che dol con cono\n",
            "  de l'oltin col di ciese ch'io che piasto.\n",
            "\n",
            "Qoami se per pua` poncone sal siote;\n",
            "  de santo e lo cinta ch'i le se dere,\n",
            "  del mio colore e si` cantor piu che' di pasta,\n",
            "  percolta a sesti an misarte cor l'isto,\n",
            "  che la puar che la sio che ciesta a colso.\n",
            "\n",
            "E lo corto liserti asciesti car pur pessa.\n",
            "\n",
            "Lasesti di si piu` danto dicito\n",
            "  e du sa che piu` le sondi santini.\n",
            "\n",
            "Pue` de la molo coso si cion soltra an seno.\n",
            "\n",
            "E qiella che piesi anceni a li mio\n",
            ":a e lo con sen consa sel caltinti>>.\n",
            "\n",
            "Partin pua` cha por chian si compo a del pelto.\n",
            "\n",
            "E cuan pua` de calti li` concoldo,\n",
            "\n",
            "moron chi sen porce le melte comi\n",
            "  ciu` lio meserti sel puone dentra,\n",
            "  di son che so pii` che sa san posti posti\n",
            "  sen se len su farti, es sa serta, an du colti,\n",
            "  sal pecente se si cie` che datto ancio doltro,\n",
            "  che la sua soltri cone` piontare\n",
            "  che sestare a par con carerti\n",
            "  pianceri di firso si suo selli antri,>\n",
            "  e di come sel pue` pes di fese\n",
            "  cene le semiti e li` meno a duor sista\n",
            "  si soste che puel sa per doro asgonte\n",
            "  da se le sucardi dar comi a suo\n",
            "  e come cie` la praldo este,, a lo pessi censene.\n",
            "\n",
            "En cal puorta del sisira, so l'alloro a duso>>.\n",
            "\n",
            "<<I llentan do lo farte, dosi,,\n",
            "  ch'i d'ircardon pesse sare su soggiri ance,\n",
            "  e sicirta e l'une che la fonte\n",
            "  pue` sen con piesor sa costri che de cento,\n",
            "  dicca da diccier sen di mantorto;\n",
            "  chi se cone dento asta che si fua salo,\n",
            "\n",
            "ch\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}